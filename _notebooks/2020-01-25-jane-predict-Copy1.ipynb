{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street Market Prediction ðŸŽ¯\n",
    "> Jane Street Market Prediction Kaggle Competition part 2\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Jaekang Lee\n",
    "- categories: [MLP, python, feature engineering, imputation, Jane Street, Kaggle, Visualization, Big Data, random forest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got a score of 9443.499 (249th place out of 3616 competitors) using MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library ðŸ“‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-wrangling/__results__.html\n",
      "/kaggle/input/data-wrangling/__notebook_source__.ipynb\n",
      "/kaggle/input/data-wrangling/imputed.csv\n",
      "/kaggle/input/data-wrangling/__notebook__.ipynb\n",
      "/kaggle/input/data-wrangling/__output__.json\n",
      "/kaggle/input/data-wrangling/one_on_top.csv\n",
      "/kaggle/input/data-wrangling/custom.css\n",
      "/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n",
      "/kaggle/input/jane-street-market-prediction/features.csv\n",
      "/kaggle/input/jane-street-market-prediction/example_test.csv\n",
      "/kaggle/input/jane-street-market-prediction/train.csv\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n",
      "/kaggle/input/janestreetimputeddata/negative_knn_1.csv\n",
      "/kaggle/input/janestreetimputeddata/negative_forward_backward.csv\n",
      "/kaggle/input/janestreetimputeddata/negative_target.csv\n",
      "/kaggle/input/janestreetimputeddata/negative_iterative_impute.csv\n",
      "/kaggle/input/janestreetimputeddata/positive_knn_5.csv\n",
      "/kaggle/input/janestreetimputeddata/positive_knn_3.csv\n",
      "/kaggle/input/janestreetimputeddata/positive_iterative_impute.csv\n",
      "/kaggle/input/janestreetimputeddata/negative_knn_3.csv\n",
      "/kaggle/input/janestreetimputeddata/positive_forward_backward.csv\n",
      "/kaggle/input/janestreetimputeddata/positive_knn_1.csv\n",
      "/kaggle/input/janestreetimputeddata/nan_neg.csv\n",
      "/kaggle/input/janestreetimputeddata/positive_soft_impute.csv\n",
      "/kaggle/input/janestreetimputeddata/nan_pos.csv\n",
      "/kaggle/input/janestreetimputeddata/negative_knn_5.csv\n",
      "/kaggle/input/janestreetimputeddata/negative_soft_impute.csv\n",
      "/kaggle/input/janestreetimputeddata/positive_target.csv\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from random import choices\n",
    "!pip install datatable > /dev/null\n",
    "import datatable as dt\n",
    "from sklearn import impute\n",
    "import gc\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "SEED = 42\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values ðŸˆ³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before in my [EDA notebook](https://leejaeka.github.io/jaekangai/python/eda/jane%20street/kaggle/visualization/big%20data/2021/01/23/JaneStreet-Copy1.html), we have couple of options to handle null values. <br>\n",
    "1. Drop all nans\n",
    "2. Impute with median or mean\n",
    "3. Feedforward/backward\n",
    "4. KNN imputer\n",
    "5. Be creative! \n",
    "<br>\n",
    "\n",
    "In this notebook, I used KNN imputer with 5 nearest neighbors to fill the nans. This takes a long time to run so I suggest downloading the imputed data files from [here](https://www.kaggle.com/louise2001/janestreetimputeddata) by louise2001. Note that he also uploaded soft and iterative imputes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "df = df.set_index('ts_id', drop=True)\n",
    "df.drop(columns=[f\"resp_{i}\" for i in range(1, 5)], inplace=True)\n",
    "print(f'Done loading data. df shape is {df.shape}')\n",
    "TARGET = 'resp'\n",
    "FEATURES = [f\"feature_{i}\" for i in range(1, 130)]\n",
    "train_pos, train_neg = df.loc[df.feature_0 > 0], df.loc[df.feature_0 < 0]\n",
    "train_pos.drop(columns=[TARGET, 'feature_0'], inplace=True)\n",
    "train_neg.drop(columns=[TARGET, 'feature_0'], inplace=True)\n",
    "gc.collect()\n",
    "nan_neg = pd.read_csv(\"../input/janestreetimputeddata/nan_neg.csv\", header=None, sep=' ').values.astype(int)\n",
    "nan_pos = pd.read_csv(\"../input/janestreetimputeddata/nan_pos.csv\", header=None, sep=' ').values.astype(int)\n",
    "\n",
    "# Split into X and y\n",
    "from copy import deepcopy as dc\n",
    "X_pos = dc(train_pos[FEATURES].values)\n",
    "X_neg = dc(train_neg[FEATURES].values)\n",
    "del train_pos, train_neg\n",
    "gc.collect()\n",
    "\n",
    "# load files \n",
    "file = 'knn_5'\n",
    "path = \"../input/janestreetimputeddata/\"\n",
    "X_pos[nan_pos[0], nan_pos[1]] = pd.read_csv(path+f\"positive_{file}.csv\", \n",
    "                                            header=None, sep=' ').values.flatten()\n",
    "X_neg[nan_neg[0], nan_neg[1]] = pd.read_csv(path+f\"negative_{file}.csv\",                                         \n",
    "                                            header=None, sep=' ').values.flatten()\n",
    "\n",
    "df = np.concatenate((X_pos, X_neg), axis=0)\n",
    "del X_pos, X_neg, nan_neg, nan_pos\n",
    "gc.collect()\n",
    "\n",
    "df = pd.DataFrame(df, columns = FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "train = dt.fread('../input/jane-street-market-prediction/train.csv')\n",
    "train = train.to_pandas()\n",
    "train = train[['date', 'weight', 'ts_id', 'resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4','feature_0']]\n",
    "gc.collect()\n",
    "\n",
    "# split train into 1s and 0s \n",
    "upper = train[train['feature_0'] == 1].sort_values(by='ts_id', axis=0, ascending=True)\n",
    "lower = train[train['feature_0'] == -1].sort_values(by='ts_id', axis=0, ascending=True)\n",
    "\n",
    "# attach\n",
    "train = pd.concat([upper, lower], axis = 0)\n",
    "del upper, lower\n",
    "gc.collect()\n",
    "\n",
    "# save files\n",
    "df.to_csv('imputed.csv', index=False)\n",
    "train.to_csv('one_on_top.csv', index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data ðŸ“š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are just going to load the imputed data instead of running the feature engineering here. Since it is very time consuming and takes a lot of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3815"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "imputed_df = dt.fread('../input/data-wrangling/imputed.csv')\n",
    "imputed_df = imputed_df.to_pandas()\n",
    "train = dt.fread('../input/data-wrangling/one_on_top.csv')\n",
    "train = train.to_pandas()\n",
    "\n",
    "df = pd.concat([train, imputed_df], axis=1, ignore_index=False)\n",
    "del train, imputed_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603878</td>\n",
       "      <td>6.086305</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>8.313583</td>\n",
       "      <td>1.782433</td>\n",
       "      <td>14.018213</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>12.600292</td>\n",
       "      <td>2.301488</td>\n",
       "      <td>11.445807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.138531</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745019</td>\n",
       "      <td>5.354213</td>\n",
       "      <td>0.344850</td>\n",
       "      <td>4.101145</td>\n",
       "      <td>0.614252</td>\n",
       "      <td>6.623456</td>\n",
       "      <td>0.800129</td>\n",
       "      <td>5.233243</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>3.926633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.116557</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>-0.007301</td>\n",
       "      <td>-0.009085</td>\n",
       "      <td>-0.001677</td>\n",
       "      <td>-0.003546</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120067</td>\n",
       "      <td>4.167835</td>\n",
       "      <td>1.537913</td>\n",
       "      <td>4.785838</td>\n",
       "      <td>1.637435</td>\n",
       "      <td>6.968002</td>\n",
       "      <td>2.354338</td>\n",
       "      <td>5.825499</td>\n",
       "      <td>1.778029</td>\n",
       "      <td>4.740577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.160117</td>\n",
       "      <td>9</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>1</td>\n",
       "      <td>2.744408</td>\n",
       "      <td>...</td>\n",
       "      <td>1.430190</td>\n",
       "      <td>3.332330</td>\n",
       "      <td>1.796860</td>\n",
       "      <td>3.177064</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>2.906432</td>\n",
       "      <td>1.589816</td>\n",
       "      <td>2.435999</td>\n",
       "      <td>1.472419</td>\n",
       "      <td>2.245991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.109651</td>\n",
       "      <td>10</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>-0.003040</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>...</td>\n",
       "      <td>1.581096</td>\n",
       "      <td>6.305170</td>\n",
       "      <td>2.324290</td>\n",
       "      <td>4.881133</td>\n",
       "      <td>2.115830</td>\n",
       "      <td>6.337250</td>\n",
       "      <td>3.059392</td>\n",
       "      <td>5.350729</td>\n",
       "      <td>2.755876</td>\n",
       "      <td>4.968388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date    weight  ts_id    resp_1    resp_2    resp_3      resp    resp_4  \\\n",
       "0     0  0.000000      0  0.009916  0.014079  0.008773  0.006270  0.001390   \n",
       "1     0  0.138531      4  0.001252  0.002165 -0.001215 -0.002604 -0.006219   \n",
       "2     0  0.116557      8 -0.005460 -0.007301 -0.009085 -0.001677 -0.003546   \n",
       "3     0  0.160117      9  0.005976  0.004345  0.023712  0.020317  0.035360   \n",
       "4     0  0.109651     10  0.006899  0.003405  0.000134 -0.000690 -0.003040   \n",
       "\n",
       "   feature_0  feature_1  ...  feature_120  feature_121  feature_122  \\\n",
       "0          1  -1.872746  ...     0.603878     6.086305     1.168391   \n",
       "1          1  -3.172026  ...     0.745019     5.354213     0.344850   \n",
       "2          1  -3.172026  ...     1.120067     4.167835     1.537913   \n",
       "3          1   2.744408  ...     1.430190     3.332330     1.796860   \n",
       "4          1  -3.172026  ...     1.581096     6.305170     2.324290   \n",
       "\n",
       "   feature_123  feature_124  feature_125  feature_126  feature_127  \\\n",
       "0     8.313583     1.782433    14.018213     2.653056    12.600292   \n",
       "1     4.101145     0.614252     6.623456     0.800129     5.233243   \n",
       "2     4.785838     1.637435     6.968002     2.354338     5.825499   \n",
       "3     3.177064     0.999252     2.906432     1.589816     2.435999   \n",
       "4     4.881133     2.115830     6.337250     3.059392     5.350729   \n",
       "\n",
       "   feature_128  feature_129  \n",
       "0     2.301488    11.445807  \n",
       "1     0.362636     3.926633  \n",
       "2     1.778029     4.740577  \n",
       "3     1.472419     2.245991  \n",
       "4     2.755876     4.968388  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering ðŸ”§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first do two feature engineering right off the bat.\n",
    "1. We are going to drop any rows with 'weight' column equal to 0. This tells us that overall gain from such trade is 0. This would be like telling machine to just guess if learned correctly. <br>\n",
    "2. To explain why we are dropping all dates before day 85 can be shown visually below. Before the day 85, we can clearly see that the trend has changed quite drastically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query('date > 85').reset_index(drop = True) \n",
    "df = df[df['weight'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only have 130 features compared to over 2 million datas. We easily make more features and avoid curse of dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1571415, 139)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# Add action column (this is our target)\n",
    "df['action'] = ((df['resp'].values) > 0).astype(int)\n",
    "\n",
    "# feature names\n",
    "features = [c for c in df.columns if \"feature\" in c]\n",
    "# resp names\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need time, date and weight anymore\n",
    "df = df.loc[:, df.columns.str.contains('feature|resp', regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do log transform and add them as new columns to the dataframe. Since performing on all features will give me out of memory error, let's do this on group_0 which has tag_0 from features.csv. For more information, check out my [EDA notebook](https://leejaeka.github.io/jaekangai/python/eda/jane%20street/kaggle/visualization/big%20data/2021/01/23/JaneStreet-Copy1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log transformation for tag groups\n",
    "tag_0_group = [9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, 123]\n",
    "for col in tag_0_group:\n",
    "    df[str('log_'+str(col))] = (df[str('feature_'+str(col))]-df[str('feature_'+str(col))].min()+1).transform(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>log_73</th>\n",
       "      <th>log_79</th>\n",
       "      <th>log_85</th>\n",
       "      <th>log_91</th>\n",
       "      <th>log_97</th>\n",
       "      <th>log_103</th>\n",
       "      <th>log_109</th>\n",
       "      <th>log_115</th>\n",
       "      <th>log_122</th>\n",
       "      <th>log_123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.151305</td>\n",
       "      <td>5.467693</td>\n",
       "      <td>-0.164505</td>\n",
       "      <td>-0.189219</td>\n",
       "      <td>0.663966</td>\n",
       "      <td>0.988896</td>\n",
       "      <td>0.661407</td>\n",
       "      <td>0.897346</td>\n",
       "      <td>2.184804</td>\n",
       "      <td>...</td>\n",
       "      <td>4.371497</td>\n",
       "      <td>4.954968</td>\n",
       "      <td>1.009198e-07</td>\n",
       "      <td>1.235292e-07</td>\n",
       "      <td>1.372731e+00</td>\n",
       "      <td>7.735990e-01</td>\n",
       "      <td>1.583237</td>\n",
       "      <td>0.994426</td>\n",
       "      <td>2.206237</td>\n",
       "      <td>2.390646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.514607</td>\n",
       "      <td>0.596214</td>\n",
       "      <td>0.324062</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.845069</td>\n",
       "      <td>0.521491</td>\n",
       "      <td>0.860309</td>\n",
       "      <td>0.595352</td>\n",
       "      <td>0.310387</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385074</td>\n",
       "      <td>4.956836</td>\n",
       "      <td>1.009198e-07</td>\n",
       "      <td>1.235292e-07</td>\n",
       "      <td>7.875868e-01</td>\n",
       "      <td>5.235099e-01</td>\n",
       "      <td>0.793093</td>\n",
       "      <td>0.487668</td>\n",
       "      <td>2.191892</td>\n",
       "      <td>2.100277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.833827</td>\n",
       "      <td>-0.049648</td>\n",
       "      <td>0.262484</td>\n",
       "      <td>0.421901</td>\n",
       "      <td>0.098124</td>\n",
       "      <td>0.171741</td>\n",
       "      <td>0.034455</td>\n",
       "      <td>0.169169</td>\n",
       "      <td>0.512029</td>\n",
       "      <td>...</td>\n",
       "      <td>4.373749</td>\n",
       "      <td>4.934122</td>\n",
       "      <td>6.493667e-01</td>\n",
       "      <td>8.441718e-01</td>\n",
       "      <td>1.314139e+00</td>\n",
       "      <td>1.969321e+00</td>\n",
       "      <td>1.542457</td>\n",
       "      <td>2.065858</td>\n",
       "      <td>1.813171</td>\n",
       "      <td>2.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>0.155047</td>\n",
       "      <td>0.343024</td>\n",
       "      <td>0.451619</td>\n",
       "      <td>0.914937</td>\n",
       "      <td>-0.596771</td>\n",
       "      <td>-0.827370</td>\n",
       "      <td>-0.974472</td>\n",
       "      <td>...</td>\n",
       "      <td>4.395633</td>\n",
       "      <td>4.958532</td>\n",
       "      <td>1.072512e+00</td>\n",
       "      <td>7.936777e-01</td>\n",
       "      <td>1.070915e-07</td>\n",
       "      <td>7.520313e-08</td>\n",
       "      <td>1.298665</td>\n",
       "      <td>0.488986</td>\n",
       "      <td>1.943198</td>\n",
       "      <td>2.112894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>0.188790</td>\n",
       "      <td>0.232964</td>\n",
       "      <td>0.500087</td>\n",
       "      <td>0.639725</td>\n",
       "      <td>-0.083674</td>\n",
       "      <td>0.019814</td>\n",
       "      <td>-4.050318</td>\n",
       "      <td>...</td>\n",
       "      <td>4.390247</td>\n",
       "      <td>4.959537</td>\n",
       "      <td>8.272507e-01</td>\n",
       "      <td>1.036085e+00</td>\n",
       "      <td>6.587185e-01</td>\n",
       "      <td>4.546515e-01</td>\n",
       "      <td>1.000972</td>\n",
       "      <td>1.028346</td>\n",
       "      <td>1.824567</td>\n",
       "      <td>2.101414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0          1   3.151305   5.467693  -0.164505  -0.189219   0.663966   \n",
       "2          1   1.514607   0.596214   0.324062   0.154730   0.845069   \n",
       "4          1  -0.833827  -0.049648   0.262484   0.421901   0.098124   \n",
       "5          1  -3.172026  -3.093182   0.155047   0.343024   0.451619   \n",
       "6          1  -3.172026  -3.093182   0.188790   0.232964   0.500087   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...    log_73    log_79  \\\n",
       "0   0.988896   0.661407   0.897346   2.184804  ...  4.371497  4.954968   \n",
       "2   0.521491   0.860309   0.595352   0.310387  ...  4.385074  4.956836   \n",
       "4   0.171741   0.034455   0.169169   0.512029  ...  4.373749  4.934122   \n",
       "5   0.914937  -0.596771  -0.827370  -0.974472  ...  4.395633  4.958532   \n",
       "6   0.639725  -0.083674   0.019814  -4.050318  ...  4.390247  4.959537   \n",
       "\n",
       "         log_85        log_91        log_97       log_103   log_109   log_115  \\\n",
       "0  1.009198e-07  1.235292e-07  1.372731e+00  7.735990e-01  1.583237  0.994426   \n",
       "2  1.009198e-07  1.235292e-07  7.875868e-01  5.235099e-01  0.793093  0.487668   \n",
       "4  6.493667e-01  8.441718e-01  1.314139e+00  1.969321e+00  1.542457  2.065858   \n",
       "5  1.072512e+00  7.936777e-01  1.070915e-07  7.520313e-08  1.298665  0.488986   \n",
       "6  8.272507e-01  1.036085e+00  6.587185e-01  4.546515e-01  1.000972  1.028346   \n",
       "\n",
       "    log_122   log_123  \n",
       "0  2.206237  2.390646  \n",
       "2  2.191892  2.100277  \n",
       "4  1.813171  2.373700  \n",
       "5  1.943198  2.112894  \n",
       "6  1.824567  2.101414  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ideas for feature engineering:\n",
    "1. aggregating categorical columns by 'tags' on features.csv\n",
    "2. count above mean, mean abs change, abs energy\n",
    "3. log transform, kurt transform and other transforms\n",
    "4. get creative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons not to do more feature engineering:\n",
    "1. We have no idea what the features represent so it might be meaningless and dangerous\n",
    "2. The dataset is really big so adding couple more columns will make me run out of memory\n",
    "3. Much slower computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data âœ‚ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use approximately 20000 data as test set. Our target value is action which we already have defined as any weight times resp above 0.(positive trades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.loc[:, df.columns.str.contains('feature|log')]\n",
    "y = np.stack([(df[c] > 0).astype('int') for c in resp_cols]).T\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  9.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 25.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=32, n_jobs=-1, verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=32, n_jobs=-1, verbose=2)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.5242761692650334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "test_pred = rnd_clf.predict(X_test)\n",
    "test_pred = np.rint(test_pred)\n",
    "test_acc = np.sum(test_pred == y_test)/(y_test.shape[0]*5)\n",
    "print(\"test accuracy: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got about 52.4% accuracy with random forest. <br>\n",
    "From the confusion matrix, we can tell that the model is having harder time predicting 0's correctly. It is actually doing a good job of classifying 1's though! So with this model, we can expect to get lots of good trades but also fail to not go for bad trades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic multiple layer perceptron with AUC(Area Under Curve) metrics. After looking at many notebooks on Kaggle, MLP seem to perform the best with short run time. Let us build one ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.5501368119630926\n"
     ]
    }
   ],
   "source": [
    "test_pred = clf.predict(X_test)\n",
    "test_pred = np.rint(test_pred)\n",
    "test_acc = np.sum(test_pred == y_test)/(y_test.shape[0]*5)\n",
    "print(\"test accuracy: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually good! Although one could say that the machine is doing slightly better than me if I was to go to Jane Street and randomly decide to 'action' on trades. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that even though we are getting only around ~55% accuracy only, this is actually considered good for trading markets. To explain this, since Jane Market has billions of money, as long as they have a positive return rate, it doesn't matter how much they lose because in the end they will gain more. It is like going to a casino knowing you have more chance of winning than losing. The more time you spend here, the more you will gain out of it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_size = 5000\n",
    "hidden_units = [(150, 150, 150), (100,100,100), (200,200,200)]\n",
    "dropout_rates = [(0.25, 0.25, 0.25, 0.25), (0.3,0.3,0.3,0.3)]\n",
    "epochs = 100\n",
    "num_columns = len(features)\n",
    "num_labels = 5\n",
    "#num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "\n",
    "mlp_CV = KerasClassifier(build_fn=create_mlp, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "param_distributions = {'hidden_units':hidden_units, 'learning_rate':[1e-3, 1e-4], \n",
    "                  'label_smoothing':[1e-2, 1e-1], 'dropout_rates':dropout_rates,\n",
    "                      'num_columns': [len(features)], 'num_labels': [5]}\n",
    "\n",
    "random_cv = RandomizedSearchCV(estimator=mlp_CV, \n",
    "                               param_distributions=param_distributions, n_iter=5,\n",
    "                               n_jobs=-1, cv=3, random_state=42)\n",
    "\n",
    "random_cv.fit(X_train, y_train, callbacks=[EarlyStopping(patience=10)])#, epochs=200, batch_size=5000)\n",
    "\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(random_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomSearch and GridSearch easily runs out of memory.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from trial and error, I've learned that with learning rate at 1e-3, model overfits quickly around at 10 with batch_size around 5000. However, the model wasn't able to learn much with less than 100 epochs. One solution is to add more layers and perceptrons which is what I did and the result 2 is the result of manual hyper param tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my final review and conclusion, check out my [blog post]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things to try/explore:\n",
    "1. Weighted training. We know that sometimes we will encounter 'monster' deals. It is crucial for the Kaggle competition to get these ones correct since these will probably outweight most other trades. So we could make model that focuses more on these heavy trades. (high weight X resp data)\n",
    "2. Split data and train multiple models. Idea is that we could split the data into two by feature_0 and maybe one model that optimizes the '1's data and another model that optimizes the '-1's data. \n",
    "3. Make much more features and explore more data (requires time and big data machines)\n",
    "4. One interesting thing I learned is that apparently, in financial, it is sometimes good to heavily overfit the model. Something to do with volatile. I've experimented with this and indeed my utility score for the competition went really high when super overfitted with epoches over 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 0.5\n",
    "\n",
    "f = np.median\n",
    "models = models[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    tag_0_group = [9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, 123]\n",
    "    for col in tag_0_group:\n",
    "        df['log_'+str(col)] = (df['feature_'+str(col)]-df['feature_'+str(col)].min()+1).transform(np.log)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "You can only iterate over `iter_test()` once.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-00d44d4b25d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjanestreet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#env = janestreet.make_env()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx_tt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36miter_test\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: You can only iterate over `iter_test()` once."
     ]
    }
   ],
   "source": [
    "import janestreet\n",
    "#env = janestreet.make_env()\n",
    "for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "    if test_df['weight'].item() > 0:\n",
    "        x_tt = test_df.loc[:, features]\n",
    "        x_tt = feature_engineering(x_tt).values\n",
    "        if np.isnan(x_tt[:, 1:].sum()):\n",
    "            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n",
    "        pred = f(pred)\n",
    "        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "    else:\n",
    "        pred_df.action = 0\n",
    "    env.predict(pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[Imputing-missing-values](https://www.kaggle.com/louise2001/imputing-missing-values) <br>\n",
    "[OWN Jane Street with Keras NN](https://www.kaggle.com/tarlannazarov/own-jane-street-with-keras-nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
